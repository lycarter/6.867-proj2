\documentclass[10pt,letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\DeclareMathOperator{\sign}{sgn}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[margin=0.5in]{geometry}
\usepackage{graphicx}
\usepackage{sidecap}
\usepackage{float}


\title{\vspace{-4ex}Regression and SVM\vspace{-3.5ex}}
\begin{document}
\newgeometry{top=0.75in,left=0.75in,right=0.75in,bottom=1.25in}
\maketitle
\vspace{-0.5em}
\begin{abstract}
Previously, we discussed various methods for polynomial regression and minimizing error in our objective function, and explored famous examples, namely ridge regression and other norms (i.e. LAD for the L1 norm). Here, we will extend those concepts to a few classification problems, through logistic regression (a natural adaptation of the polynomial regression we've been doing) and then more formally through the concept of support vector machines (SVMs).
\end{abstract}

\section{Logistic Regression}
In past discussions, we've often encountered the objective function of our MLE estimate, or of some other estimate that we've established for some parameter $\theta$ that we'd like to minimize so as to minimize the value of our error function. This is called \textbf{regression}. Here we will discuss \textbf{logistic regression}, which involves fitting the data to a curve of the form
$$\sigma(x) = \frac{1}{1+e^{-x}}$$
This is known as the \textbf{sigmoid} function, and it's often used when the data we would like to fit is well-modeled as a classification problem (with labels +1 and -1). Given $d$-dimensional data $x^{(1)}, x^{(2)}, ... , x^{(n)}$ and corresponding scalar labels $y^{(1)}, y^{(2)}, ... , y^{(n)}$, we can define our LR objection function as:
$$\text{NLL}(w) = \sum_{i=0}^n\log(1+e^{-y^{(i)}(x^{(i)}\cdot w+w_0)})$$
given the parameters $w, w_0$. To reduce overfitting, we can apply an $L2$ regularization term to our equation, as in ridge regression. As a result, we are simply minimizing
$$E_{LR}(w) = \text{NLL}(w)+\lambda w^\intercal w$$
so that
$$w^* = \text{argmin}_w \left[\sum_{i=0}^n\log(1+e^{-y^{(i)}(x^{(i)}\cdot w+w_0)}) + \lambda w^\intercal w\right]$$

We can estimate $w$ numerically using stochastic gradient descent; if we let $\lambda = 0$, then we have no regularization and we end up with this data, as well as figures 1-8:
\begin{table}[h]
\centering
\caption{$\lambda=0$ (step = .001, threshold = .008)}
\begin{tabular}{r|cc|cc|cc|cc}
   & \multicolumn{2}{|c}{\texttt{stdev1}} & \multicolumn{2}{|c}{\texttt{stdev2}} & \multicolumn{2}{|c}{\texttt{stdev4}} & \multicolumn{2}{|c}{\texttt{data\_nonsep}}\\
$w_{init}$& \#it         & Loss        & \#it   & Loss &
\#it   & Loss           & \#it        & Loss      \\\hline
$[1, 1, 1]$  & 255           &    0        & 91      & 0.09             & 49    & 0.32           & 37            & 0.48        \\
$[4, 4, 4]$  & 466           & 0         & 259     & 0.12             & 91      & 0.36            & 64           & 0.44      \\
$[1, 10, 0]$ & 408           &  0         & 354      & 0.11              &  39     & 0.33           & 32            & 0.53      \\
$[-5, -5, -5]$ & 299         &  0        & 199     & 0.1               &  89     & 0.33            & 73           & 0.49    
\end{tabular}
\end{table}

However, if we vary $\lambda$, then we obtain more stable solutions with respect to $w$, and the solutions are more constant regardless of the starting paramters. Figure 9-12 show the decision bounday for $\lambda = 20$.
\begin{figure}[!htb]
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd1t0.png}
  \caption{\texttt{stdev = 1}}\label{fig:gradDifQ}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd2t0.png}
  \caption{\texttt{stdev = 2}}\label{fig:gradDifN}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd4t0.png}
  \caption{\texttt{stdev = 4}}\label{fig:gradDifS}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sdnt0.png}
  \caption{\texttt{unseparable}}\label{fig:gradDifS}
\endminipage
\end{figure}
\begin{figure}[!htb]
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd1v0.png}
  \caption{\texttt{stdev = 1}}\label{fig:gradDifQ}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd2v0.png}
  \caption{\texttt{stdev = 2}}\label{fig:gradDifN}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd4v0.png}
  \caption{\texttt{stdev = 4}}\label{fig:gradDifS}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sdnv0.png}
  \caption{\texttt{unseparable}}\label{fig:gradDifS}
\endminipage

\end{figure}


\begin{figure}[!htb]
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd1v2.png}
  \caption{\texttt{stdev = 1}}\label{fig:gradDifQ}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd2v20.png}
  \caption{\texttt{stdev = 2}}\label{fig:gradDifN}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sd4v20.png}
  \caption{\texttt{stdev = 4}}\label{fig:gradDifS}
\endminipage\hfill
\minipage{0.25\textwidth}
  \includegraphics[width=\linewidth]{figures/sdnv20.png}
  \caption{\texttt{unseparable}}\label{fig:gradDifS}
\endminipage

\end{figure}

Figures 1-4 show the training data, and figures 5-8 show the validation data for $\lambda = 0$. The predictor is reasonably effective and seems to seek to minimize the loss. When we increase $\lambda$, however, we instead increase the loss as we try to minimize the $\lambda w^\intercal w$ term in the error function. This means that the model is more inaccurate in this case, which is because with two parameters, $w$ wasn't likely going to overfit to the data anyways. But note that the effect is not too large, even for larger $\lambda$ ($\lambda = 40$) - our main goal is still to minimize the total error.

\section{Support Vector Machines}
Support Vector Machines are a machine-learning technique that is used to classify binary sets of data. We define $\theta$ as the normal to the decision hyperplane, and classify data points by $\sign (\theta^\intercal x + \theta_0)$. In order to train the classifier, we initially set up the minimization problem (known as \textbf{Hard-SVM}):

\begin{equation}
\max_{\theta, \theta_0} \dfrac{1}{||\theta||} \min_{1 \le i \le n} y^{(i)} (\theta^\intercal x^{(i)} + \theta_0)
\end{equation}

However, we cannot satisfy this minimization if the training set is not linearly separable. Therefore, we add a ``slack variable'' to each constraint, which is a measure of the ``wrongness'' of the decision boundary when classifying that point. We call these slack variables $\xi_i$. We then desire to minimize

\begin{align}
\min_{\theta, \theta_0, \xi} \dfrac{\lambda}{2} ||\theta||^2 + \dfrac{1}{n} \sum_{i=1}^{n} \xi_{i} &, \\
s.t. \hspace{.75em} y^{(i)}(\theta^\intercal x^{(i)} + \theta_0) \ge 1 - \xi_i &, \\
\xi_i \ge 0, i \in n &
\end{align}

However, we find it more computationally efficient to solve the dual form of the \textbf{Soft-SVM}:
\begin{align}
\max_{\alpha \in \mathbb{R}^n} \left[ \sum_{i=1}^n \alpha_i - \dfrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)})^\intercal x^{(j)}\right] & \\
s.t. \hspace{.75em} 0 \le \alpha_i \le C & \\
\sum_i \alpha_i y^{(i)} = 0 &
\end{align}

To provide some intuition: $\alpha_i$ is the weight of each training point on the final decision boundary. It is 0 for all $x^{(i)}$ that are farther from the decision boundary than the margin, so that only the ``important'' $x^{(i)}$ are ``support vectors''. $C$ is the maximum value of $\alpha$, and determines the size of the margin. \textbf{ADD SOMETHING ABOUT WHICH WAY THAT WORKS HERE}. To return from $\alpha$ to the more familiar $\theta$ and $\theta_0$, we plug in:

\begin{align}
\theta &= \sum_{i=1}^n \alpha_i y^{(i)} x^{(i)} \\
\theta_0 &= \dfrac{1}{\mathcal{M}} \left[ \sum_{j \in \mathcal{M}} \left( y^{(j)} - \sum_{i \in \mathcal{S}} \alpha_i y^{(i)} (x^{(j)})^\intercal x^{(i)} \right) \right]
\end{align}

Where $\mathcal{M} = \{ i : 0 < \alpha_i < C \}$ and $\mathcal{S} = \{ i : 0 < \alpha_i \}$.

For example, given the data $X = [1, 2;\ 2, 2;\ 0, 0;\ -2, 3], Y = [1;\ 1;\ -1;\ -1]$, we generate the following objective function:

\begin{equation}
\min_{\alpha \in \mathbb{R}^n} \left[\dfrac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y^{(i)} y^{(j)} (x^{(i)})^\intercal x^{(j)} - \sum_{i=1}^n \alpha_i \right]
\end{equation}

This can be reformulated to be plugged into a standard quadratic programming solver, with \textbf{blah blah blah}

\begin{center}
\begin{figure}[!htb]
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/C,01sigma1.png}
  \caption{$C = 0.01, \sigma = 1$}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/C,1sigma1.png}
  \caption{$C = 0.1, \sigma = 1$}
\endminipage\hfill
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/C1sigma1.png}
  \caption{$C = 1, \sigma = 1$}
\endminipage
\end{figure}
\begin{figure}[!htb]
\hspace{.16\textwidth}
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/C10sigma1.png}
  \caption{$C = 10, \sigma = 1$}
\endminipage
\minipage{0.32\textwidth}
  \includegraphics[width=\linewidth]{figures/C100sigma1.png}
  \caption{$C = 100, \sigma = 1$}
\endminipage
\end{figure}
\end{center}

We also note that as $C$ increases, the number of boundary points decreases. Thus, the number of support vectors decrease. For the nonseparable example, for $C = (.01, .1, 1, 10, 100)$, we see that $\mathcal{M} = (400, 324, 133, 68, 59)$.

As per the usual procedure with the approximately equivalent $\lambda$, to select the correct $C$, we first compute $\alpha$ for various values of $C$ using a training set, then determine the validation error using a validation set. We select the $C$ which produces the lowest validation error in order to maximize the generality of our solution.


\section{Titanic Data}
\end{document}













